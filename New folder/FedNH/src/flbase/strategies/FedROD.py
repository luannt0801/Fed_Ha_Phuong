from collections import OrderedDict, Counter
import numpy as np
from tqdm import tqdm
from copy import deepcopy
import torch
try:
    import wandb
except ModuleNotFoundError:
    pass
from ..server import Server
from ..client import Client
from ..model import ModelWrapper
from ..models.CNN import *
from ..models.MLP import *
from ..strategies.FedAvg import FedAvgServer
from ..utils import setup_optimizer, linear_combination_state_dict, setup_seed
from ...utils import autoassign, save_to_pkl, access_last_added_element
import time
import torch
import torch.nn as nn
import torch.nn.functional as F


class HyperClassifier(nn.Module):
    def __init__(self, feature_dim, num_classes):
        super(HyperClassifier, self).__init__()
        self.num_classes = num_classes
        self.feature_dim = feature_dim
        self.out_size = (feature_dim, num_classes)
        self.linear1 = nn.Linear(self.num_classes, self.feature_dim)
        self.linear2 = nn.Linear(self.feature_dim, np.prod(self.out_size))

    def forward(self, label_dist):
        # y is a place holder
        h_in = F.relu(self.linear1(label_dist))
        h_final = self.linear2(h_in)
        # reshape as a matrix
        h_final = h_final.view(self.out_size)
        return h_final


class FedRODClient(Client):
    def __init__(self, criterion, trainset, testset, client_config, cid, device, **kwargs):
        super().__init__(criterion, trainset, testset,
                         client_config, cid, device, **kwargs)
        self.is_on_server = False
        # prepare for balanced softmax loss
        temp = [self.count_by_class[cls] if cls in self.count_by_class.keys() else 1e-12 for cls in range(client_config['num_classes'])]
        count_by_class_full = torch.tensor(temp).to(self.device)
        self.sample_per_class = count_by_class_full / torch.sum(count_by_class_full)

        self._initialize_model()

    def set_on_server(self):
        self.is_on_server = True

    def _initialize_model(self):
        # parse the model from config file
        self.model = eval(self.client_config['model'])(self.client_config).to(self.device)
        # separate base and head
        g_head = deepcopy(self.model.prototype)
        if self.client_config['FedROD_hyper_clf']:
            num_classes, feature_dim = g_head.out_features, g_head.in_features
            hypercls = HyperClassifier(feature_dim, num_classes)
            # add for aggregation
            self.model.hypercls = hypercls.to(self.device)
            self.p_head = None
        else:
            # now self.p_head is a nn.Linear
            self.p_head = deepcopy(g_head)
        self.model.prototype = nn.Identity()
        self.model = ModelWrapper(self.model, g_head, self.model.config)
        # this is needed if the criterion has stateful tensors.
        self.criterion = self.criterion.to(self.device)

    def training(self, round, num_epochs):
        if self.client_config['FedROD_phead_separate']:
            self._train_separate(round, num_epochs)
        else:
            self._train_together(round, num_epochs)

    def _train_together(self, round, num_epochs):
        """
            Note that in order to use the latest server side model the `set_params` method should be called before `training` method.
        """
        setup_seed(round + self.client_config['global_seed'])
        # train mode
        self.model.train()
        # tracking stats
        self.num_rounds_particiapted += 1
        loss_seq = []
        acc_seq = []
        if self.trainloader is None:
            raise ValueError("No trainloader is provided!")
        optimizer = setup_optimizer(self.model, self.client_config, round)
        optimizer_lr = optimizer.param_groups[0]['lr']
        if self.client_config['FedROD_hyper_clf']:
            # p_head is a simple linear layer
            p_head_optimizer = torch.optim.SGD(self.model.base.hypercls.parameters(), lr=optimizer_lr)
        else:
            # p_head is generated by a hypernet work
            p_head_optimizer = torch.optim.SGD(self.p_head.parameters(), lr=optimizer_lr)

        for i in range(num_epochs):
            epoch_loss, correct = 0.0, 0
            for j, (x, y) in enumerate(self.trainloader):
                # forward pass
                x, y = x.to(self.device), y.to(self.device)
                embedding, out_g = self.model(x, return_embedding=True)
                loss_bsm = balanced_softmax_loss(y, out_g, self.sample_per_class)
                self.model.zero_grad(set_to_none=True)
                loss_bsm.backward()
                torch.nn.utils.clip_grad_norm_(parameters=filter(lambda p: p.requires_grad, self.model.parameters()), max_norm=10)
                optimizer.step()
                # train p head
                if self.client_config['FedROD_hyper_clf']:
                    # generate p head
                    # now self.p_head is a torch.Tensor
                    self.p_head = self.model.base.hypercls(self.sample_per_class)
                    out_p = torch.matmul(embedding.detach(), self.p_head)
                else:
                    out_p = self.p_head(embedding.detach())

                out = out_p + out_g.detach()
                loss = self.criterion(out, y)
                p_head_optimizer.zero_grad()
                loss.backward()
                p_head_optimizer.step()

                predicted = out.data.max(1)[1]
                correct += predicted.eq(y.data).sum().item()
                epoch_loss += loss.item() * x.shape[0]  # rescale to bacthsize
            epoch_loss /= len(self.trainloader.dataset)
            epoch_accuracy = correct / len(self.trainloader.dataset)
            loss_seq.append(epoch_loss)
            acc_seq.append(epoch_accuracy)
        self.new_state_dict = self.model.state_dict()
        self.train_loss_dict[round] = loss_seq
        self.train_acc_dict[round] = acc_seq

    def _train_separate(self, round, num_epochs):
        """
            Note that in order to use the latest server side model the `set_params` method should be called before `training` method.
        """
        setup_seed(round + self.client_config['global_seed'])
        # train mode
        self.model.train()
        # tracking stats
        self.num_rounds_particiapted += 1
        loss_seq = []
        acc_seq = []
        if self.trainloader is None:
            raise ValueError("No trainloader is provided!")
        optimizer = setup_optimizer(self.model, self.client_config, round)
        optimizer_lr = optimizer.param_groups[0]['lr']
        if self.client_config['FedROD_hyper_clf']:
            # p_head is a simple linear layer
            p_head_optimizer = torch.optim.SGD(self.model.base.hypercls.parameters(), lr=optimizer_lr)
        else:
            # p_head is generated by a hypernet work
            p_head_optimizer = torch.optim.SGD(self.p_head.parameters(), lr=optimizer_lr)
        for i in range(num_epochs):
            # t_start = time.time()
            for j, (x, y) in enumerate(self.trainloader):
                # forward pass
                x, y = x.to(self.device), y.to(self.device)
                embedding, out_g = self.model(x, return_embedding=True)
                loss_bsm = balanced_softmax_loss(y, out_g, self.sample_per_class)
                self.model.zero_grad(set_to_none=True)
                loss_bsm.backward()
                torch.nn.utils.clip_grad_norm_(parameters=filter(lambda p: p.requires_grad, self.model.parameters()), max_norm=10)
                optimizer.step()
                # t_end = time.time() - t_start
            # print(i, '/', num_epochs, "train shared part time", t_end, 'samples', self.num_train_samples)
        # exit()

        """
        Trian Hypernetwork - phead
        """
        for i in range(num_epochs):
            epoch_loss, correct = 0.0, 0
            for j, (x, y) in enumerate(self.trainloader):
                # forward pass
                x, y = x.to(self.device), y.to(self.device)
                with torch.no_grad():
                    embedding, out_g = self.model(x, return_embedding=True)
                # train p head
                if self.client_config['FedROD_hyper_clf']:
                    # generate p head
                    # now self.p_head is a torch.Tensor
                    self.p_head = self.model.base.hypercls(self.sample_per_class)
                    out_p = torch.matmul(embedding.detach(), self.p_head)
                else:
                    out_p = self.p_head(embedding.detach())

                out = out_p + out_g.detach()
                loss = self.criterion(out, y)
                p_head_optimizer.zero_grad()
                loss.backward()
                p_head_optimizer.step()

                predicted = out.data.max(1)[1]
                correct += predicted.eq(y.data).sum().item()
                epoch_loss += loss.item() * x.shape[0]  # rescale to bacthsize
            epoch_loss /= len(self.trainloader.dataset)
            epoch_accuracy = correct / len(self.trainloader.dataset)
            loss_seq.append(epoch_loss)
            acc_seq.append(epoch_accuracy)
        # t_end = time.time() - t_start
        # print("train client time", t_end, 'samples', self.num_train_samples)
        # exit()
        self.new_state_dict = self.model.state_dict()
        self.train_loss_dict[round] = loss_seq
        self.train_acc_dict[round] = acc_seq

    def upload(self):
        return self.new_state_dict

    def testing(self, round, testloader=None):
        self.model.eval()
        if testloader is None:
            testloader = self.testloader
        test_count_per_class = Counter(testloader.dataset.targets.numpy())
        # all_classes_sorted = sorted(test_count_per_class.keys())
        # test_count_per_class = torch.tensor([test_count_per_class[cls] * 1.0 for cls in all_classes_sorted])
        # num_classes = len(all_classes_sorted)
        num_classes = self.client_config['num_classes']
        test_count_per_class = torch.tensor([test_count_per_class[cls] * 1.0 for cls in range(num_classes)])
        test_correct_per_class = torch.tensor([0] * num_classes)

        weight_per_class_dict = {'uniform': torch.tensor([1.0] * num_classes),
                                 'validclass': torch.tensor([0.0] * num_classes),
                                 'labeldist': torch.tensor([0.0] * num_classes)}
        for cls in self.label_dist.keys():
            weight_per_class_dict['labeldist'][cls] = self.label_dist[cls]
            weight_per_class_dict['validclass'][cls] = 1.0
        # start testing
        with torch.no_grad():
            for i, (x, y) in enumerate(testloader):
                # forward pass
                x, y = x.to(self.device), y.to(self.device)
                embedding, out_g = self.model(x, return_embedding=True)
                if self.is_on_server:
                    out = out_g
                else:
                    if self.client_config['FedROD_hyper_clf']:
                        out_p = torch.matmul(embedding.detach(), self.p_head)
                    else:
                        out_p = self.p_head(embedding.detach())
                    out = out_p + out_g
                # stats
                predicted = out.data.max(1)[1]
                classes_shown_in_this_batch = torch.unique(y).cpu().numpy()
                for cls in classes_shown_in_this_batch:
                    test_correct_per_class[cls] += ((predicted == y) * (y == cls)).sum().item()
        acc_by_critertia_dict = {}
        for k in weight_per_class_dict.keys():
            acc_by_critertia_dict[k] = (((weight_per_class_dict[k] * test_correct_per_class).sum()) /
                                        ((weight_per_class_dict[k] * test_count_per_class).sum())).item()

        self.test_acc_dict[round] = {'acc_by_criteria': acc_by_critertia_dict,
                                     'correct_per_class': test_correct_per_class,
                                     'weight_per_class': weight_per_class_dict}


class FedRODServer(FedAvgServer):
    def __init__(self, server_config, clients_dict, exclude, **kwargs):
        super().__init__(server_config, clients_dict, exclude, **kwargs)
        # set correct status so that it use the global model to perform evaluation
        self.server_side_client.set_on_server()

# https://github.com/jiawei-ren/BalancedMetaSoftmax-Classification


def balanced_softmax_loss(labels, logits, sample_per_class, reduction="mean"):
    """Compute the Balanced Softmax Loss between `logits` and the ground truth `labels`.
    Args:
      labels: A int tensor of size [batch].
      logits: A float tensor of size [batch, no_of_classes].
      sample_per_class: A int tensor of size [no of classes].
      reduction: string. One of "none", "mean", "sum"
    Returns:
      loss: A float tensor. Balanced Softmax Loss.
    """
    spc = sample_per_class.type_as(logits)
    spc = spc.unsqueeze(0).expand(logits.shape[0], -1)
    logits = logits + spc.log()
    loss = F.cross_entropy(input=logits, target=labels, reduction=reduction)
    return loss
